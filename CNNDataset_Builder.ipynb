{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "iuk8Ipu8Gntp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS8Ph3wBGMJQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from rasterio.windows import bounds # Correct import\n",
        "import glob\n",
        "import os\n",
        "import pyproj\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your Sentinel-2 .SAFE.zip file.\n",
        "zip_path = '/content/S2B_MSIL2A_20250405T042659_N0511_R133_T46QBM_20250405T063356.SAFE.zip'\n",
        "\n",
        "# Directory where you want to extract the files.\n",
        "extract_path = '/content/extracted_data'\n",
        "\n",
        "# Create the extraction directory if it doesn't already exist.\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Open the zip file and extract all its contents.\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Extraction complete. Files are extracted to: {extract_path}\")"
      ],
      "metadata": {
        "id": "WeVEo4IGGjMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kiln_locations=pd.read_excel('/content/all_kilns_dec102024.xlsx')"
      ],
      "metadata": {
        "id": "M2-ybZUMGmf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safe_dir = '/content/extracted_data/S2B_MSIL2A_20250405T042659_N0511_R133_T46QBM_20250405T063356.SAFE'\n",
        "\n",
        "patch_size = 64\n",
        "stride = 64\n",
        "SENTINEL_SCALE_FACTOR = 10000.0\n",
        "random_seed = 42\n",
        "\n",
        "\n",
        "output_dir = \"/content/prepared_datasets_balanced_1_1\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "train_output_file = os.path.join(output_dir, \"kiln_train_balanced_1_1.npz\")\n",
        "val_output_file = os.path.join(output_dir, \"kiln_val_balanced_1_1.npz\")\n",
        "test_output_file = os.path.join(output_dir, \"kiln_test_balanced_1_1.npz\")\n",
        "\n",
        "# Step 1: Extract Patches and Metadata\n",
        "print(\"--- Starting Patch Extraction & Metadata Collection ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    band_paths = [\n",
        "        glob.glob(os.path.join(safe_dir, 'GRANULE', '*', 'IMG_DATA', 'R10m', '*_B04_10m.jp2'))[0], # Red\n",
        "        glob.glob(os.path.join(safe_dir, 'GRANULE', '*', 'IMG_DATA', 'R10m', '*_B03_10m.jp2'))[0], # Green\n",
        "        glob.glob(os.path.join(safe_dir, 'GRANULE', '*', 'IMG_DATA', 'R10m', '*_B02_10m.jp2'))[0], # Blue\n",
        "    ]\n",
        "    print(\"Located band files.\")\n",
        "except IndexError:\n",
        "    print(f\"Error: Could not find one or more band files in {safe_dir} structure.\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred locating band files: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "srcs = [rasterio.open(path) for path in band_paths]\n",
        "transform = srcs[0].transform\n",
        "crs = srcs[0].crs\n",
        "width = srcs[0].width\n",
        "height = srcs[0].height\n",
        "\n",
        "try:\n",
        "    to_latlon = pyproj.Transformer.from_crs(crs, \"EPSG:4326\", always_xy=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing pyproj Transformer: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "if 'kiln_locations' not in locals() or not isinstance(kiln_locations, pd.DataFrame):\n",
        "     print(\"Error: kiln_locations DataFrame not found or not loaded.\")\n",
        "     sys.exit(1)\n",
        "\n",
        "all_patch_data_list = []\n",
        "all_patch_metadata_list = []\n",
        "\n",
        "print(f\"Extracting patches...\")\n",
        "# Loop over the image by patch\n",
        "for i, (row_offset, col_offset) in enumerate(\n",
        "    [(r, c) for r in range(0, height - patch_size + 1, stride)\n",
        "            for c in range(0, width - patch_size + 1, stride)]\n",
        "):\n",
        "    window = Window(col_off=col_offset, row_off=row_offset, width=patch_size, height=patch_size)\n",
        "    patch_data = np.zeros((patch_size, patch_size, len(srcs)), dtype=np.float32)\n",
        "    valid_bands = True\n",
        "    for band_idx, src in enumerate(srcs):\n",
        "        band_data = src.read(1, window=window).astype(np.float32)\n",
        "        if band_data.size == 0 or np.all(band_data == 0):\n",
        "             valid_bands = False\n",
        "             break\n",
        "        scaled_band_data = (band_data / SENTINEL_SCALE_FACTOR) * 255.0\n",
        "        scaled_band_data = np.clip(scaled_band_data, 0, 255)\n",
        "        patch_data[..., band_idx] = scaled_band_data\n",
        "\n",
        "    if not valid_bands or np.all(patch_data == 0):\n",
        "        continue\n",
        "\n",
        "    patch_bounds_coords = bounds(window, transform=transform)\n",
        "    left, bottom, right, top = patch_bounds_coords\n",
        "    lon_left, lat_bottom = to_latlon.transform(left, bottom)\n",
        "    lon_right, lat_top = to_latlon.transform(right, top)\n",
        "\n",
        "    all_patch_data_list.append(patch_data)\n",
        "    all_patch_metadata_list.append({\n",
        "        \"lon_left\": lon_left, \"lat_bottom\": lat_bottom,\n",
        "        \"lon_right\": lon_right, \"lat_top\": lat_top\n",
        "    })\n",
        "\n",
        "    if (i + 1) % 5000 == 0:\n",
        "         print(f\"  Extracted {i + 1} potential patches...\")\n",
        "\n",
        "for src in srcs:\n",
        "    src.close()\n",
        "\n",
        "if not all_patch_data_list:\n",
        "    print(\"Error: No valid patches were extracted. Exiting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"Finished extraction. Created {len(all_patch_data_list)} valid patches.\")\n",
        "print(f\"Patch extraction took: {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "#  Step 2: Convert Patch Data to NumPy Array\n",
        "print(\"\\nConverting patch data to NumPy array...\")\n",
        "start_time = time.time()\n",
        "X_all = np.array(all_patch_data_list)\n",
        "del all_patch_data_list # Free memory\n",
        "print(f\"Conversion took: {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"X_all shape: {X_all.shape}\")\n",
        "\n",
        "#  Step 3: Create Metadata DataFrame & Assign Labels\n",
        "print(\"\\nCreating metadata DataFrame and assigning labels...\")\n",
        "start_time = time.time()\n",
        "df_all_metadata = pd.DataFrame(all_patch_metadata_list)\n",
        "df_all_metadata['patch_index'] = df_all_metadata.index\n",
        "\n",
        "y_all = np.zeros(len(df_all_metadata), dtype=np.int32)\n",
        "\n",
        "kiln_patch_indices = set()\n",
        "print(f\"Matching {len(kiln_locations)} kiln locations to patches...\")\n",
        "for _, kiln_row in kiln_locations.iterrows():\n",
        "    kiln_lon = kiln_row[\"Longitude\"]\n",
        "    kiln_lat = kiln_row[\"Latitude\"]\n",
        "    contained_in = df_all_metadata[\n",
        "        (df_all_metadata['lon_left'] <= kiln_lon) & (df_all_metadata['lon_right'] >= kiln_lon) &\n",
        "        (df_all_metadata['lat_bottom'] <= kiln_lat) & (df_all_metadata['lat_top'] >= kiln_lat)\n",
        "    ]\n",
        "    kiln_patch_indices.update(contained_in['patch_index'].tolist())\n",
        "\n",
        "if kiln_patch_indices:\n",
        "    y_all[list(kiln_patch_indices)] = 1\n",
        "\n",
        "print(f\"Label assignment took: {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"y_all shape: {y_all.shape}\")\n",
        "unique_all, counts_all = np.unique(y_all, return_counts=True)\n",
        "print(f\"Overall class distribution: {dict(zip(unique_all, counts_all))}\")\n",
        "\n",
        "#  Step 4: Balance the Entire Dataset (1:1 Undersampling)\n",
        "print(\"\\nPerforming 1:1 undersampling on the full dataset...\")\n",
        "start_time = time.time()\n",
        "\n",
        "indices_class_0 = np.where(y_all == 0)[0]\n",
        "indices_class_1 = np.where(y_all == 1)[0]\n",
        "\n",
        "n_class_0_all = len(indices_class_0)\n",
        "n_class_1_all = len(indices_class_1)\n",
        "\n",
        "if n_class_1_all == 0:\n",
        "     print(\"Error: No kiln samples found in the dataset. Cannot balance.\")\n",
        "     sys.exit(1)\n",
        "\n",
        "n_majority_keep = n_class_1_all\n",
        "\n",
        "if n_class_0_all < n_majority_keep:\n",
        "    print(f\"Warning: Fewer non-kiln samples ({n_class_0_all}) than kiln samples ({n_class_1_all}). \"\n",
        "          f\"Using all non-kiln samples and sampling kiln samples down.\")\n",
        "    n_majority_keep = n_class_0_all\n",
        "    selected_minority_indices = np.random.choice(indices_class_1, size=n_majority_keep, replace=False, seed=random_seed)\n",
        "    selected_majority_indices = indices_class_0\n",
        "else:\n",
        "    print(f\"Keeping all {n_class_1_all} kiln samples.\")\n",
        "    print(f\"Randomly selecting {n_majority_keep} non-kiln samples for a 1:1 balance.\")\n",
        "    np.random.seed(random_seed)\n",
        "    selected_majority_indices = np.random.choice(indices_class_0, size=n_majority_keep, replace=False)\n",
        "    selected_minority_indices = indices_class_1\n",
        "\n",
        "balanced_indices = np.concatenate([selected_minority_indices, selected_majority_indices])\n",
        "\n",
        "X_balanced = X_all[balanced_indices]\n",
        "y_balanced = y_all[balanced_indices]\n",
        "\n",
        "X_balanced, y_balanced = shuffle(X_balanced, y_balanced, random_state=random_seed)\n",
        "\n",
        "print(f\"Balancing took: {time.time() - start_time:.2f} seconds.\")\n",
        "print(f\"\\nTotal BALANCED dataset shape: X={X_balanced.shape}, y={y_balanced.shape}\")\n",
        "unique_balanced, counts_balanced = np.unique(y_balanced, return_counts=True)\n",
        "print(f\"Total BALANCED dataset class distribution: {dict(zip(unique_balanced, counts_balanced))}\")\n",
        "\n",
        "del X_all\n",
        "del y_all\n",
        "\n",
        "#  Step 5: Split Balanced Data into Train/Validation/Test Sets\n",
        "print(\"\\nSplitting BALANCED data into Train (70%), Validation (15%), Test (15%)...\")\n",
        "\n",
        "# First split: Train (70%) and Temp (30%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_balanced, y_balanced,\n",
        "    test_size=0.30,\n",
        "    random_state=random_seed,\n",
        "    stratify=y_balanced\n",
        ")\n",
        "\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=random_seed,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"\\nFinal Dataset Shapes:\")\n",
        "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Val:   X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "# Check distribution in splits (should be roughly 50/50)\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "print(f\"  Train distribution: {dict(zip(unique_train, counts_train))}\")\n",
        "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
        "print(f\"  Val distribution:   {dict(zip(unique_val, counts_val))}\")\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(f\"  Test distribution:  {dict(zip(unique_test, counts_test))}\")\n",
        "\n",
        "#  Step 6: Save the Final Datasets\n",
        "print(\"\\n--- Saving Final Datasets ---\")\n",
        "try:\n",
        "    print(f\"Saving BALANCED training set to {train_output_file}...\")\n",
        "    np.savez_compressed(train_output_file, X_train=X_train, y_train=y_train)\n",
        "\n",
        "    print(f\"Saving BALANCED validation set to {val_output_file}...\")\n",
        "    np.savez_compressed(val_output_file, X_val=X_val, y_val=y_val)\n",
        "\n",
        "    print(f\"Saving BALANCED test set to {test_output_file}...\")\n",
        "    np.savez_compressed(test_output_file, X_test=X_test, y_test=y_test)\n",
        "\n",
        "    print(\"\\nAll dataset splits saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving dataset files: {e}\")\n",
        "\n",
        "print(\"\\nPreprocessing and dataset creation complete.\")"
      ],
      "metadata": {
        "id": "MBsuspadGUCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}